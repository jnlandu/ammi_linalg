\begin{frame}{Eigendecomposition/Diagonalization}
    \begin{itemize}
        \item Consider a matrix $A$ possessing $n$ eigenvalues $\lambda_1, \ldots, \lambda_n$ and $n$ linearly independent eigenvectors $x_1, \ldots, x_n$. We can construct:
        \begin{itemize}
            \item Diagonal matrix: $D=\text{diag}(\lambda_1, \ldots, \lambda_n)$
            \item Matrix of eigenvectors: $P=[x_1 \; x_2 \; \cdots \; x_n]$
        \end{itemize}
        This allows us to represent $A$ as:
         \begin{align}
         A=P D P^{-1} =  P \begin{bmatrix}
        \lambda_1 & 0 & \cdots & 0 \\
        0 & \lambda_2 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & \lambda_n
        \end{bmatrix} P^{-1}
         \end{align}
         \item A matrix that allows such a decomposition is termed "diagonalizable". \textbf{A matrix is diagonalizable if and only if it can be expressed as $A = PDP^{-1}$.}
     \end{itemize}
    \end{frame}
    \begin{frame}{Example}
    \begin{itemize}
         \item \textbf{All symmetric matrices are diagonalizable.} The spectral theorem guarantees the existence of a matrix $P$ such that:
        \begin{align}
            A= PDP^{-1} \label{eqbon2,2}
        \end{align}
        \item Consider the matrix $A$:
        \[
        A = \begin{bmatrix}
        3 & 1 \\
        1 & 3
        \end{bmatrix}
        \]
        \item We aim to find the eigendecomposition $A = P DP^{-1}$, where $P$ has eigenvectors as columns and $D$ is the diagonal matrix of corresponding eigenvalues.
    \end{itemize}
    \end{frame}


    \begin{frame}{}
    \begin{itemize}
    \item  \textbf{Step 1: Find the Eigenvalues}. Solve \( \det(A - \lambda I) = 0 \):
    \begin{align*}
    p_A(\lambda) = \det(A - \lambda I) =\det\begin{bmatrix}
    3 - \lambda & 1 \\
    1 & 3 - \lambda
    \end{bmatrix} & = (3 - \lambda)^2 - 1 \\
    &  = \lambda^2 - 6\lambda + 8=(\lambda - 4)(\lambda - 2)
    \end{align*}
    \item The eigenvalues are: \( \sigma(A) = \{\lambda_1 = 4, \lambda_2 = 2 \}\).
     \item \textbf{Step 2: Find the Eigenvectors}

    For \( \lambda_1 = 4 \):

    Solve \( (A - 4I)\begin{bmatrix} x \\ y \end{bmatrix}  = 0  \Longleftrightarrow \begin{bmatrix} -1 & 1 \\ 1 & -1 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = 0 \). This yields:
    \[
    -x + y = 0 \implies y = x. \text{ Thus: } \; v_1 =\begin{bmatrix} 1 \\ 1 \end{bmatrix} 
    \]
    \end{itemize}
    \end{frame}

    \begin{frame}{}
    \begin{itemize}
    \item 
    For \( \lambda_2 = 2 \):

    Solve \( (A - 2I )\begin{bmatrix} x \\ y \end{bmatrix}  = 0 \Longleftrightarrow\begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = 0 \). This gives:
    \[
    x + y = 0 \implies y = -x. \text{ Thus: } \; v_2=\begin{bmatrix} 1 \\ -1 \end{bmatrix}
    \]
    \item  \textbf{Step 3: Form $D$ and $P$:}
        \begin{align*}
           P = \begin{bmatrix}
        1 & 1 \\
        1 & -1
        \end{bmatrix}, \quad D = \begin{bmatrix}
        4 & 0 \\
        0 & 2
        \end{bmatrix} 
        \end{align*}
    \end{itemize}
    \end{frame}


    \begin{frame}{Conditions for Diagonalization}
    \begin{itemize}
    \item  Not all matrices are symmetric. We must therefore establish conditions for a matrix to be diagonalizable. Let $A$ be an $n\times n$ matrix.
      \item \textbf{Condition 1:} \( A \) is diagonalizable if there exists a basis of \( \mathbb{R}^n \) composed of eigenvectors of \( A \), i.e., \textbf{\( A \) possesses \( n \) linearly independent eigenvectors.}

        \item \textbf{Condition 2:} If  $A$ \textbf{has \( n \) distinct eigenvalues}, then it is diagonalizable (since eigenvectors corresponding to distinct eigenvalues are linearly independent).
        \item \textbf{Condition 3: }  $A$ is diagonalizable if   \textbf{$A$ satisfies  $GM=AM$.}
    \end{itemize}
    \end{frame}
    \begin{frame}{Exercises: Diagonalizability }
    \begin{itemize}
        \item Determine which of the following matrices are diagonalizable over \( \mathbb{R} \). Provide justification for your answer.
    \end{itemize}

    \vspace{0.5em}
    \begin{align*}
        \text{a)}\;\; A &= \begin{bmatrix}
            1 & 0 \\
            0 & 0
        \end{bmatrix} 
        \qquad 
        \text{b)}\;\; B = \begin{bmatrix}
            2 & 1 \\
            0 & 2
        \end{bmatrix} 
        \qquad 
        \text{c)}\;\; C = \begin{bmatrix}
            0 & 1 \\
            -1 & 0
        \end{bmatrix} \\[1.5em]
        \text{d)}\;\; D &= \begin{bmatrix}
            5 & 4 & 2 \\
            0 & 1 & -1 \\
            0 & 0 & 3
        \end{bmatrix}
        \qquad 
        \text{e)}\;\; E = \begin{bmatrix}
            4 & 1 & 1 \\
            1 & 4 & 1 \\
            1 & 1 & 4
        \end{bmatrix} 
        \qquad 
        \text{f)}\;\; F = \begin{bmatrix}
            0 & 1 & 0 \\
            0 & 0 & 1 \\
            0 & 0 & 0
        \end{bmatrix}
    \end{align*}
    \end{frame}



\subsection{Singular Value Decomposition}
\begin{frame}{SVD Theorem}
\begin{itemize}
    \item Let \( A \in \mathbb{R}^{m \times n} \) be a matrix of rank \( r \leq \min(m, n) \). The Singular Value Decomposition (SVD) of \( A \) expresses it as:

\[
A = U \Sigma V^T
\]

where:
\begin{itemize}
    \item \( U \in \mathbb{R}^{m \times m} \) is an orthogonal matrix with columns \( u_i \) (left singular vectors)
    \item \( V \in \mathbb{R}^{n \times n} \) is an orthogonal matrix with columns \( v_j \) (right singular vectors)
    \item \( \Sigma \in \mathbb{R}^{m \times n} \) has diagonal entries \( \sigma_i \geq 0 \) and zeros elsewhere
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Properties of SVD}
\begin{itemize}
    \item The diagonal entries $\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_r > 0$ are called \textbf{singular values} of $A$.
    \item The columns $u_i$ are \textbf{left singular vectors}, and columns $v_j$ are \textbf{right singular vectors}.
    \item The singular values are conventionally ordered in decreasing order.
    \item The matrix $\Sigma$ is uniquely determined. For $m > n$:
    \begin{align}
        \Sigma = \begin{bmatrix}
            \sigma_1 & 0 & \cdots & 0 \\
            0 & \sigma_2 & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & \sigma_n \\
            0 & 0 & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & 0
        \end{bmatrix}
    \end{align}
\end{itemize}
\end{frame}

\begin{frame}{}
\begin{itemize}
    \item For $m < n$, $\Sigma$ has the structure:
    \begin{align}
        \Sigma = \begin{bmatrix}
            \sigma_1 & 0 & \cdots & 0 & 0 & \cdots & 0 \\
            0 & \sigma_2 & \cdots & 0 & 0 & \cdots & 0\\
            \vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & \sigma_m & 0 & \cdots & 0
        \end{bmatrix}
    \end{align}
    \item \textbf{Key fact:} The SVD exists for \emph{any} matrix $A \in \mathbb{R}^{m \times n}$, regardless of rank or shape.
\end{itemize}
\end{frame}

\begin{frame}{Computing the SVD}
\begin{itemize}
    \item The SVD can be constructed using the eigendecomposition of $A^TA$:
    \begin{align}
        A^TA = Q\Lambda Q^T = Q \begin{bmatrix}
            \lambda_1 & \cdots & 0\\
            \vdots & \ddots & \vdots\\
            0 & \cdots & \lambda_n
        \end{bmatrix} Q^T
    \end{align}
    where $Q$ contains orthonormal eigenvectors and $\lambda_i \geq 0$ are eigenvalues of $A^TA$.
    \item Starting from the SVD form $A = U\Sigma V^T$, we compute:
    \begin{align}
        A^TA = (U\Sigma V^T)^T (U\Sigma V^T) = V\Sigma^T U^T U \Sigma V^T
    \end{align}
\end{itemize}
\end{frame}

\begin{frame}{}
\begin{itemize}
    \item Since $U$ is orthogonal ($U^TU = I$), we get:
    \begin{align}
        A^TA = V\Sigma^T \Sigma V^T = V \begin{bmatrix}
            \sigma_1^2 & & \\
            & \ddots & \\
            & & \sigma_n^2
        \end{bmatrix} V^T
    \end{align}
    \item This shows that the singular values are $\sigma_i = \sqrt{\lambda_i}$, where $\lambda_i$ are eigenvalues of $A^TA$.
\end{itemize}
\end{frame}
